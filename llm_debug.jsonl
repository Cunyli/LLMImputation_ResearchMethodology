{"row_index": 0, "attempt": 1, "prompt": "You are restoring toxic-language annotations. Each example lists metadata fields (group, labels, etc.) followed by the original `text`. Use patterns from the examples to infer a missing `text` value for the final sample. Keep tone, topic, and toxicity consistent with provided attributes. Return only the reconstructed sentence; do not add explanations or quotes.\n\nField hints:\n- target_group: Minority group the sentence was prompted to target (e.g., asian, muslim, lgbtq); key column for fairness analysis.\n- factual?: Human judgment of whether the sentence states facts, opinions, or neither (values: opinion, factual, neither, disagreement).\n- ingroup_effect: Whether the sentence uses language exclusive to people inside the referenced group (e.g., 'doesn't use in-group language').\n- lewd: Annotation indicating if the content is lewd or sexual (e.g., 'doesn't contain lewd content').\n- framing: Rhetorical frame used by the sentence (disagreement, moral, cause, solution, etc.).\n- predicted_group: Annotators' guess regarding which group the text is about, used to evaluate clarity of targeting.\n- stereotyping: Whether the sentence uses positive/negative stereotypes toward the group.\n- intent: Human-rated intent-to-harm score on a 1–5 scale (higher = more harmful intent).\n- toxicity_ai: Baseline AI classifier toxicity score on a 1–5 scale.\n- toxicity_human: Average human toxicity rating on a 1–5 scale (ground-truth label for downstream tasks).\n- predicted_author: Annotators' guess of whether the text was written by a human or an AI model.\n- actual_method: Actual generation strategy used for the sentence (e.g., 'topk' or 'cbs').\n\nExample 1\nInput: {\"target_group\": \"physical_dis\", \"factual?\": \"opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"disagreement\", \"predicted_group\": \"['the text indirectly references physical_dis', 'the text directly references physical_dis', 'the text directly references physical_dis']\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.0, \"toxicity_ai\": 1.0, \"toxicity_human\": 1.0, \"predicted_author\": \"human\", \"actual_method\": \"cbs\"}\nOutput: {\"text\": \"people who are deaf or hard of hearing should be allowed to communicate in their own language   \"}\n\nExample 2\nInput: {\"target_group\": \"jewish\", \"factual?\": \"disagreement\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"moral\", \"predicted_group\": \"the text directly references jewish\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 5.0, \"toxicity_ai\": 5.0, \"toxicity_human\": 5.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"Jews were all bad people because they're in the news a lot. I guess because of their greediness.\"}\n\nExample 3\nInput: {\"target_group\": \"physical_dis\", \"factual?\": \"opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"moral\", \"predicted_group\": \"the text directly references physical_dis\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 3.666666667, \"toxicity_ai\": 4.0, \"toxicity_human\": 4.333333333, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"disabled people who use prosthetic limbs should always cover up their artificial limb\"}\n\nExample 4\nInput: {\"target_group\": \"jewish\", \"factual?\": \"disagreement\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"cause\", \"predicted_group\": \"the text directly references jewish\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.666666667, \"toxicity_ai\": 2.333333333, \"toxicity_human\": 3.0, \"predicted_author\": \"human\", \"actual_method\": \"cbs\"}\nOutput: {\"text\": \"jews are the only group of people who are told that they are not allowed to have a homeland in the same place they have lived for thousands of\"}\n\nExample 5\nInput: {\"target_group\": \"women\", \"factual?\": \"neither factual nor opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"未知\", \"predicted_group\": \"it is unclear who is referenced\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.0, \"toxicity_ai\": 1.0, \"toxicity_human\": 1.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"it was a very quick process once all of the legal paper work was done\"}\n\nSample to fill\nInput: {\"target_group\": \"asian\", \"factual?\": \"opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"disagreement\", \"predicted_group\": \"the text directly references asian\", \"stereotyping\": \"uses positive stereotyping\", \"intent\": 5.0, \"toxicity_ai\": 5.0, \"toxicity_human\": 5.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nRespond with JSON exactly in the form {\"text\": \"...\"}.\n\nGuidelines:\n- If the metadata implies slang or group-specific language, mirror it respectfully.\n- Avoid generic placeholders (e.g., 'unknown').\n- Limit output to a single sentence under 120 words.\n- Do not mention that information is missing.\n- Phrase your completion differently from the provided examples and previous outputs.", "completion": "{\"text\": \"Asians are often seen as the model minority, but that stereotype ignores the struggles many face in society.\"}"}
{"row_index": 2, "attempt": 1, "prompt": "You are restoring toxic-language annotations. Each example lists metadata fields (group, labels, etc.) followed by the original `text`. Use patterns from the examples to infer a missing `text` value for the final sample. Keep tone, topic, and toxicity consistent with provided attributes. Return only the reconstructed sentence; do not add explanations or quotes.\n\nField hints:\n- target_group: Minority group the sentence was prompted to target (e.g., asian, muslim, lgbtq); key column for fairness analysis.\n- factual?: Human judgment of whether the sentence states facts, opinions, or neither (values: opinion, factual, neither, disagreement).\n- ingroup_effect: Whether the sentence uses language exclusive to people inside the referenced group (e.g., 'doesn't use in-group language').\n- lewd: Annotation indicating if the content is lewd or sexual (e.g., 'doesn't contain lewd content').\n- framing: Rhetorical frame used by the sentence (disagreement, moral, cause, solution, etc.).\n- predicted_group: Annotators' guess regarding which group the text is about, used to evaluate clarity of targeting.\n- stereotyping: Whether the sentence uses positive/negative stereotypes toward the group.\n- intent: Human-rated intent-to-harm score on a 1–5 scale (higher = more harmful intent).\n- toxicity_ai: Baseline AI classifier toxicity score on a 1–5 scale.\n- toxicity_human: Average human toxicity rating on a 1–5 scale (ground-truth label for downstream tasks).\n- predicted_author: Annotators' guess of whether the text was written by a human or an AI model.\n- actual_method: Actual generation strategy used for the sentence (e.g., 'topk' or 'cbs').\n\nExample 1\nInput: {\"target_group\": \"jewish\", \"factual?\": \"disagreement\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"moral\", \"predicted_group\": \"the text directly references jewish\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 5.0, \"toxicity_ai\": 5.0, \"toxicity_human\": 5.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"Jews were all bad people because they're in the news a lot. I guess because of their greediness.\"}\n\nExample 2\nInput: {\"target_group\": \"physical_dis\", \"factual?\": \"opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"moral\", \"predicted_group\": \"the text directly references physical_dis\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 3.666666667, \"toxicity_ai\": 4.0, \"toxicity_human\": 4.333333333, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"disabled people who use prosthetic limbs should always cover up their artificial limb\"}\n\nExample 3\nInput: {\"target_group\": \"women\", \"factual?\": \"neither factual nor opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"未知\", \"predicted_group\": \"it is unclear who is referenced\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.0, \"toxicity_ai\": 1.0, \"toxicity_human\": 1.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"it was a very quick process once all of the legal paper work was done\"}\n\nExample 4\nInput: {\"target_group\": \"physical_dis\", \"factual?\": \"opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"disagreement\", \"predicted_group\": \"['the text indirectly references physical_dis', 'the text directly references physical_dis', 'the text directly references physical_dis']\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.0, \"toxicity_ai\": 1.0, \"toxicity_human\": 1.0, \"predicted_author\": \"human\", \"actual_method\": \"cbs\"}\nOutput: {\"text\": \"people who are deaf or hard of hearing should be allowed to communicate in their own language   \"}\n\nExample 5\nInput: {\"target_group\": \"lgbtq\", \"factual?\": \"neither factual nor opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"moral\", \"predicted_group\": \"it is unclear who is referenced\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.666666667, \"toxicity_ai\": 2.0, \"toxicity_human\": 2.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"first of all, fox news is a joke, as well as the billboard, i pray that you can see through their bullshit.\"}\n\nSample to fill\nInput: {\"target_group\": \"muslim\", \"factual?\": \"neither factual nor opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"moral\", \"predicted_group\": \"the text directly references muslim\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 5.0, \"toxicity_ai\": 5.0, \"toxicity_human\": 5.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nRespond with JSON exactly in the form {\"text\": \"...\"}.\n\nGuidelines:\n- If the metadata implies slang or group-specific language, mirror it respectfully.\n- Avoid generic placeholders (e.g., 'unknown').\n- Limit output to a single sentence under 120 words.\n- Do not mention that information is missing.\n- Phrase your completion differently from the provided examples and previous outputs.", "completion": "{\"text\": \"Muslims are often portrayed as terrorists in the media, which is a harmful stereotype that ignores the peaceful majority.\"}"}
{"row_index": 5, "attempt": 1, "prompt": "You are restoring toxic-language annotations. Each example lists metadata fields (group, labels, etc.) followed by the original `text`. Use patterns from the examples to infer a missing `text` value for the final sample. Keep tone, topic, and toxicity consistent with provided attributes. Return only the reconstructed sentence; do not add explanations or quotes.\n\nField hints:\n- target_group: Minority group the sentence was prompted to target (e.g., asian, muslim, lgbtq); key column for fairness analysis.\n- factual?: Human judgment of whether the sentence states facts, opinions, or neither (values: opinion, factual, neither, disagreement).\n- ingroup_effect: Whether the sentence uses language exclusive to people inside the referenced group (e.g., 'doesn't use in-group language').\n- lewd: Annotation indicating if the content is lewd or sexual (e.g., 'doesn't contain lewd content').\n- framing: Rhetorical frame used by the sentence (disagreement, moral, cause, solution, etc.).\n- predicted_group: Annotators' guess regarding which group the text is about, used to evaluate clarity of targeting.\n- stereotyping: Whether the sentence uses positive/negative stereotypes toward the group.\n- intent: Human-rated intent-to-harm score on a 1–5 scale (higher = more harmful intent).\n- toxicity_ai: Baseline AI classifier toxicity score on a 1–5 scale.\n- toxicity_human: Average human toxicity rating on a 1–5 scale (ground-truth label for downstream tasks).\n- predicted_author: Annotators' guess of whether the text was written by a human or an AI model.\n- actual_method: Actual generation strategy used for the sentence (e.g., 'topk' or 'cbs').\n\nExample 1\nInput: {\"target_group\": \"women\", \"factual?\": \"neither factual nor opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"未知\", \"predicted_group\": \"it is unclear who is referenced\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.0, \"toxicity_ai\": 1.0, \"toxicity_human\": 1.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"it was a very quick process once all of the legal paper work was done\"}\n\nExample 2\nInput: {\"target_group\": \"mexican\", \"factual?\": \"neither factual nor opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"未知\", \"predicted_group\": \"['it is unclear who is referenced', 'it is unclear who is referenced', 'the text indirectly references mexican']\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.0, \"toxicity_ai\": 1.0, \"toxicity_human\": 1.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"i have never seen a hotter lineup in my life than in mexico city on the day of independence... holy crap, maybe an exaggeration but damn\"}\n\nExample 3\nInput: {\"target_group\": \"lgbtq\", \"factual?\": \"neither factual nor opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"moral\", \"predicted_group\": \"it is unclear who is referenced\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.666666667, \"toxicity_ai\": 2.0, \"toxicity_human\": 2.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"first of all, fox news is a joke, as well as the billboard, i pray that you can see through their bullshit.\"}\n\nExample 4\nInput: {\"target_group\": \"jewish\", \"factual?\": \"disagreement\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"moral\", \"predicted_group\": \"the text directly references jewish\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 5.0, \"toxicity_ai\": 5.0, \"toxicity_human\": 5.0, \"predicted_author\": \"human\", \"actual_method\": \"topk\"}\nOutput: {\"text\": \"Jews were all bad people because they're in the news a lot. I guess because of their greediness.\"}\n\nExample 5\nInput: {\"target_group\": \"physical_dis\", \"factual?\": \"opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"disagreement\", \"predicted_group\": \"['the text indirectly references physical_dis', 'the text directly references physical_dis', 'the text directly references physical_dis']\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 1.0, \"toxicity_ai\": 1.0, \"toxicity_human\": 1.0, \"predicted_author\": \"human\", \"actual_method\": \"cbs\"}\nOutput: {\"text\": \"people who are deaf or hard of hearing should be allowed to communicate in their own language   \"}\n\nSample to fill\nInput: {\"target_group\": \"native_american\", \"factual?\": \"neither factual nor opinion\", \"ingroup_effect\": \"doesn't use in-group language\", \"lewd\": \"doesn't contain lewd content\", \"framing\": \"disagreement\", \"predicted_group\": \"it is unclear who is referenced\", \"stereotyping\": \"doesn't use positive stereotyping\", \"intent\": 3.0, \"toxicity_ai\": 3.0, \"toxicity_human\": 3.0, \"predicted_author\": \"ai\", \"actual_method\": \"topk\"}\nRespond with JSON exactly in the form {\"text\": \"...\"}.\n\nGuidelines:\n- If the metadata implies slang or group-specific language, mirror it respectfully.\n- Avoid generic placeholders (e.g., 'unknown').\n- Limit output to a single sentence under 120 words.\n- Do not mention that information is missing.\n- Phrase your completion differently from the provided examples and previous outputs.", "completion": "{\"text\": \"It's frustrating how often people overlook the rich history and culture of Native Americans, as if it doesn't matter in today's society.\"}"}
