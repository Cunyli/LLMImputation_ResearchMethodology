{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d549b08a",
   "metadata": {},
   "source": [
    "# DistilBERT Regression Fine-Tuning (LLM Imputation Study)\n",
    "\n",
    "This notebook fine-tunes `distilbert-base-uncased` to predict the human toxicity score (`toxicity_human`) on any imputed dataset.\n",
    "\n",
    "> **GPU required**: When running on Kaggle/Colab, enable GPU in the runtime settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5185e8bc",
   "metadata": {},
   "source": [
    "## 0. Environment Setup\n",
    "- Enable GPU (Kaggle: *Settings → Accelerators → GPU*).\n",
    "- Install required packages (Transformers ≥ 4.40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68af2df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets evaluate accelerate scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f7d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e852442",
   "metadata": {},
   "source": [
    "## 1. Configuration\n",
    "Update these paths to match your runtime environment.\n",
    "\n",
    "- `DATA_ROOT`: directory containing `train.parquet`, `train_text_imputed_*.parquet`, etc.\n",
    "- `IMPUTED_FILE`: choose which imputed training set to use.\n",
    "- `TEST_FILE`: optional test parquet; set to `None` if unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdd0df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = Path('/kaggle/input/llmimputation/data/parquet')  # TODO: update path\n",
    "IMPUTED_FILE = 'train_text_imputed_mar_knn_30.parquet'        # TODO: set desired dataset\n",
    "TEST_FILE = 'test.parquet'                                   # set to None if unavailable\n",
    "TEXT_COLUMN = 'text'\n",
    "LABEL_COLUMN = 'toxicity_human'\n",
    "LABEL_THRESHOLDS = [1.5, 2.5, 3.5, 4.5]  # for macro F1 calculation\n",
    "RANDOM_STATE = 42\n",
    "VALID_SIZE = 0.1  # validation ratio if no external validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97dc09e",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d899205",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = DATA_ROOT / IMPUTED_FILE\n",
    "assert train_path.exists(), f'Missing train file: {train_path}'\n",
    "\n",
    "df_train = pd.read_parquet(train_path)\n",
    "print(f'Train rows: {len(df_train):,}')\n",
    "\n",
    "df_test = None\n",
    "if TEST_FILE:\n",
    "    test_path = DATA_ROOT / TEST_FILE\n",
    "    if test_path.exists():\n",
    "        df_test = pd.read_parquet(test_path)\n",
    "        print(f'Test rows: {len(df_test):,}')\n",
    "    else:\n",
    "        print(f\"Warning: TEST_FILE '{TEST_FILE}' not found; skipping test split.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b27b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train[TEXT_COLUMN].astype(str)",
    "y = df_train[LABEL_COLUMN].astype(np.float32)",
    "",
    "X_train, X_valid, y_train, y_valid = train_test_split(",
    "    X,",
    "    y,",
    "    test_size=VALID_SIZE,",
    "    random_state=RANDOM_STATE,",
    "    stratify=np.digitize(y, LABEL_THRESHOLDS),",
    ")",
    "",
    "train_dataset = Dataset.from_dict({TEXT_COLUMN: X_train, LABEL_COLUMN: y_train})",
    "valid_dataset = Dataset.from_dict({TEXT_COLUMN: X_valid, LABEL_COLUMN: y_valid})",
    "",
    "datasets_dict = DatasetDict({'train': train_dataset, 'validation': valid_dataset})",
    "",
    "if df_test is not None:",
    "    test_dataset = Dataset.from_dict({",
    "        TEXT_COLUMN: df_test[TEXT_COLUMN].astype(str),",
    "        LABEL_COLUMN: df_test[LABEL_COLUMN].astype(np.float32),",
    "    })",
    "    datasets_dict['test'] = test_dataset",
    "",
    "display(datasets_dict)",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feb3089",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[TEXT_COLUMN],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "    )\n",
    "    model_inputs['labels'] = example[LABEL_COLUMN]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = datasets_dict.map(\n",
    "    preprocess,\n",
    "    batched=True,\n",
    "    remove_columns=[TEXT_COLUMN, LABEL_COLUMN]\n",
    ")\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c6833",
   "metadata": {},
   "source": [
    "## 4. Metrics\n",
    "We optimise MSE but also report MAE, RMSE, Pearson/Spearman correlations, and macro F1 by discretising predictions with the same thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = predictions.reshape(-1)\n",
    "    labels = labels.reshape(-1)\n",
    "\n",
    "    mae = mean_absolute_error(labels, predictions)\n",
    "    rmse = mean_squared_error(labels, predictions, squared=False)\n",
    "    pearson = stats.pearsonr(labels, predictions).statistic\n",
    "    spearman = stats.spearmanr(labels, predictions).statistic\n",
    "\n",
    "    true_bins = np.digitize(labels, LABEL_THRESHOLDS)\n",
    "    pred_bins = np.digitize(predictions, LABEL_THRESHOLDS)\n",
    "    macro_f1 = f1_score(true_bins, pred_bins, average='macro')\n",
    "\n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'pearson': pearson,\n",
    "        'spearman': spearman,\n",
    "        'macro_f1_from_regression': macro_f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e965c8de",
   "metadata": {},
   "source": [
    "## 5. Trainer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=1,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "batch_size = 16  # larger batch for DistilBERT\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 3\n",
    "warmup_ratio = 0.1\n",
    "learning_rate = 3e-5\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./distilbert_regression_outputs',\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='eval_rmse',\n",
    "    greater_is_better=False,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to='none',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e22a2c3",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133111fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = trainer.train()\n",
    "trainer.save_model('distilbert_regression_best')  # saves tokenizer + model\n",
    "train_result.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788649c",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff068f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = trainer.evaluate(tokenized_datasets['validation'])\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c50d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test' in tokenized_datasets:\n",
    "    test_metrics = trainer.evaluate(tokenized_datasets['test'], metric_key_prefix='test')\n",
    "    test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a4de98",
   "metadata": {},
   "source": [
    "## 8. Save Metrics & Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1620010",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict_to_json(data, path):\n",
    "    import json\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "save_dir = Path('./distilbert_regression_outputs')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "save_dict_to_json(eval_metrics, save_dir / 'eval_metrics.json')\n",
    "\n",
    "predictions = trainer.predict(tokenized_datasets['validation'])\n",
    "np.savetxt(\n",
    "    save_dir / 'validation_predictions.csv',\n",
    "    np.vstack([predictions.predictions.reshape(-1), predictions.label_ids]).T,\n",
    "    delimiter=',',\n",
    "    header='pred,label',\n",
    "    comments=''\n",
    ")\n",
    "print('Saved metrics and validation predictions to', save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}